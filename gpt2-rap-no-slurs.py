# -*- coding: utf-8 -*-
"""gpt2-rap-no-slurs.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1aYProaH6kJyRVmllCvcbEo656J8a52ng
"""


from transformers import GPT2LMHeadModel, GPT2Tokenizer
import torch
from transformers import Trainer, TrainingArguments
import torch
from tqdm import tqdm
from lyrics import Lyrics
import pandas as pd
from sklearn.model_selection import train_test_split
from transformers import TextDataset, DataCollatorForLanguageModeling
import numpy as np

DEVICE = torch.device("cuda:0")

model_name_or_path = "gpt2"
tokenizer = GPT2Tokenizer.from_pretrained(model_name_or_path)
model = GPT2LMHeadModel.from_pretrained(model_name_or_path, use_cache=False).to(DEVICE)


corpus_path = './gpt2_rap_nosl_f2.txt'

with open(corpus_path) as tr:
    lyrics = tr.read()
    lyrics = lyrics.split("<|endoftext|>")
train, test = train_test_split(lyrics, test_size=0.01)
train, test = pd.DataFrame(train), pd.DataFrame(test)
train_path = "./train_prp.csv"
test_path = "./test_prp.csv"
train.to_csv(train_path, index=False)
test.to_csv(test_path, index=False)



train_dataset = TextDataset(tokenizer=tokenizer,file_path=train_path,block_size=64, 
                            overwrite_cache=True)
test_dataset = TextDataset(tokenizer=tokenizer,file_path=test_path,block_size=64, 
                            overwrite_cache=True)
data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)



training_args = TrainingArguments( 
    output_dir= "./finetuned",
    overwrite_output_dir=True,
    num_train_epochs=3, 
    per_device_train_batch_size=32, 
    per_device_eval_batch_size=32,  
    gradient_accumulation_steps=16, 
    )


trainer = Trainer(
    model=model,
    args=training_args,
    data_collator=data_collator,
    train_dataset=train_dataset,
    eval_dataset=test_dataset,
    optimizers = (torch.optim.AdamW(model.parameters(),lr=1e-5),None)
)

trainer.train()

encodings = tokenizer("".join(list(test[0])), return_tensors="pt")


max_length = model.config.n_positions
stride = 512
seq_len = encodings.input_ids.size(1)

nlls = []
prev_end_loc = 0
for begin_loc in tqdm(range(0, seq_len, stride)):
    end_loc = min(begin_loc + max_length, seq_len)
    trg_len = end_loc - prev_end_loc  # may be different from stride on last loop
    input_ids = encodings.input_ids[:, begin_loc:end_loc].to(DEVICE)
    target_ids = input_ids.clone()
    target_ids[:, :-trg_len] = -100

    with torch.no_grad():
        outputs = model(input_ids, labels=target_ids)

        # loss is calculated using CrossEntropyLoss which averages over input tokens.
        # Multiply it with trg_len to get the summation instead of average.
        # We will take average over all the tokens to get the true average
        # in the last step of this example.
        neg_log_likelihood = outputs.loss * trg_len

    nlls.append(neg_log_likelihood)

    prev_end_loc = end_loc
    if end_loc == seq_len:
        break

ppl = torch.exp(torch.stack(nlls).sum() / end_loc)
print("Perplexity", ppl)

text = list(test[0])
testset = [t.split("Next Line:") for t in text]
print(testset[0])


# measure rhyme density on test set
generated_rls = []
test_rls = []
with open("generated_text.txt", "w") as generated:
    for l in testset:
        line, next_line = l[0], l[1]
        input_ids = tokenizer.encode(line, return_tensors="pt").to(DEVICE)
        model.eval()
        with torch.no_grad():
            out = model.generate(input_ids, 
                                do_sample=True,
                                temperature=1.4,
                                top_k=50,
                                max_length=50,
                                )

        generated_text = list(map(tokenizer.decode, out))[0]
        generated.write(generated_text+"\n")
        l = Lyrics(text=generated_text, print_stats=False, 
                language="english")
        rl = l.get_avg_rhyme_length()
        generated_rls.append(rl)
        l = Lyrics(text=next_line, print_stats=False, 
                language="english")
        rl = l.get_avg_rhyme_length()
        test_rls.append(rl)
    print(np.mean(generated_rls), np.mean(test_rls))
# 0.6591951787184029 0.3025075757575757

print("Rhyme density of generated text",np.mean(generated_rls))
print("Rhyme density of test set text",np.mean(test_rls))

model_dir = './'
trainer.save_model(model_dir + 'model/')

input_ids = tokenizer.encode(text, return_tensors="pt").to(DEVICE)
model.eval()
with torch.no_grad():
    out = model.generate(input_ids, 
                        do_sample=True,
                        temperature=1.4,
                        top_k=50,
                        max_length=50,
                        )

generated_text = list(map(tokenizer.decode, out))[0]
print(generated_text)

"""-------------------------------------------------------------------

Rhyme density measurements
"""

#!sudo apt-get install espeak -y